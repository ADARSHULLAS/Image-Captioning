{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VisionEncoderDecoderModel, AdamW, get_scheduler, AutoTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'Using GPU: {device}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", add_special_tokens=True)\n",
    "\n",
    "if decoder_tokenizer.pad_token is None:\n",
    "    decoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, caption_dir, tokenizer):\n",
    "        self.image_dir = image_dir\n",
    "        self.caption_dir = caption_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.caption_files = os.listdir(caption_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_file = self.image_files[index]\n",
    "        caption_file = f\"{image_file.split('.')[0]}_{index % 4}.txt\"\n",
    "\n",
    "        image_tensor = torch.load(os.path.join(self.image_dir, image_file)).to(device)\n",
    "\n",
    "        with open(os.path.join(self.caption_dir, caption_file), 'r') as f:\n",
    "            caption_token = [int(token) for token in f.read().split()]\n",
    "\n",
    "        return {\"pixel_values\": image_tensor, \"caption_token\": torch.tensor(caption_token)}\n",
    "\n",
    "# Define the collate function\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    caption_tokens = [item[\"caption_token\"] for item in batch]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    caption_tokens = pad_sequence(caption_tokens, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"caption_token\": caption_tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = ImageCaptionDataset('D:\\MS COCO\\preprocessed_images', 'D:\\MS COCO\\preprocessed_captions', decoder_tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"D:\\Final\\enthavumo entho\\Save_at_50_epochs.pt\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs and batch size\n",
    "num_epochs = 10\n",
    "batch_size = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=int(num_training_steps/5), num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        batch['pixel_values'] = batch['pixel_values'].view(batch['pixel_values'].size(0), 3, 224, 224)\n",
    "        outputs = model(pixel_values=batch['pixel_values'], labels=batch['caption_token'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {sum(losses)/len(losses)}')\n",
    "\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        model.save_pretrained(f\"Save_at_{epoch+1}_epochs.pt\")\n",
    "        print(f'Saved model at {epoch+1} epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, model, tokenizer, device):\n",
    "    # Convert image to tensor and move it to the appropriate device\n",
    "    image = torch.unsqueeze(torch.tensor(image), 0).to(device)\n",
    "    \n",
    "    # Encode image using the model's encoder\n",
    "    encoder_output = model.encoder(pixel_values=image)\n",
    "    \n",
    "    # Initialize the decoder input with the special token for start of sequence\n",
    "    decoder_input_ids = torch.tensor(tokenizer.encode(\"[CLS]\")).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize the list to hold generated token IDs\n",
    "    generated_ids = []\n",
    "    \n",
    "    # Set maximum length for generated caption\n",
    "    max_length = 32\n",
    "    \n",
    "    # Generate tokens one by one using the decoder\n",
    "    for _ in range(max_length):\n",
    "        # Generate next token\n",
    "        outputs = model.decoder(input_ids=decoder_input_ids, encoder_hidden_states=encoder_output.last_hidden_state)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_id = next_token_logits.argmax(1).unsqueeze(-1)\n",
    "        \n",
    "        # Append the token to the list of generated tokens\n",
    "        generated_ids.append(next_token_id.item())\n",
    "        \n",
    "        # Break if the end of sequence token is generated\n",
    "        if next_token_id.item() == tokenizer.sep_token_id:\n",
    "            break\n",
    "        \n",
    "        # Prepare input for the next iteration\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=-1)\n",
    "    \n",
    "    # Decode the generated token IDs into a caption string\n",
    "    generated_caption = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Display the image and print the generated caption\n",
    "    image = np.moveaxis(image[0].cpu().numpy(), 0, -1)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(\"Generated Caption:\", generated_caption)\n",
    "    \n",
    "    return generated_caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"/kaggle/input/model50-pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(i)['pixel_values'].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset( image_path='/kaggle/input/impoleds/', text_df=text_data_train , tokenizer=decoder_tokenizer, image_height=224, image_width=224 )\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_folder = '/kaggle/input/impoleds/'\n",
    "image_files = ['im1.png','im2.png','im3.png','im4.png','im5.png','im6.png','im7.png','im8.png','im14.png','im9.jpg','im10.jpg','im11.jpg','im12.jpg','im13.jpg','im15.jpg','im16.jpg']\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((224, 224))\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    if image_array.shape[2] == 4:\n",
    "        image_array = image_array[:, :, :3]\n",
    "    elif image_array.shape[2] == 1:\n",
    "        image_array = np.repeat(image_array, 3, axis=2)\n",
    "    image_array = np.transpose(image_array, (2, 0, 1))\n",
    "\n",
    "    pred_caption = generate_caption(\n",
    "        image=image_array,\n",
    "        model=model.to('cuda'),\n",
    "        tokenizer=decoder_tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
